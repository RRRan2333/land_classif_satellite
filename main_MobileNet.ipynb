{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "from keras import layers, models, optimizers, regularizers\n",
    "from keras.applications import MobileNet \n",
    "from keras.applications.efficientnet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if running the GPU version of tensorflow\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads directory of 3 datasets\n",
    "dir_train = pd.read_csv('dataset/EuroSAT/train.csv')\n",
    "dir_valid = pd.read_csv('dataset/EuroSAT/validation.csv')\n",
    "dir_test = pd.read_csv('dataset/EuroSAT/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24300, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Label</th>\n",
       "      <th>ClassName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AnnualCrop/AnnualCrop_142.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>AnnualCrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HerbaceousVegetation/HerbaceousVegetation_2835...</td>\n",
       "      <td>2</td>\n",
       "      <td>HerbaceousVegetation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PermanentCrop/PermanentCrop_1073.jpg</td>\n",
       "      <td>6</td>\n",
       "      <td>PermanentCrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Industrial/Industrial_453.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HerbaceousVegetation/HerbaceousVegetation_1810...</td>\n",
       "      <td>2</td>\n",
       "      <td>HerbaceousVegetation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24295</th>\n",
       "      <td>SeaLake/SeaLake_1943.jpg</td>\n",
       "      <td>9</td>\n",
       "      <td>SeaLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24296</th>\n",
       "      <td>AnnualCrop/AnnualCrop_211.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>AnnualCrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24297</th>\n",
       "      <td>Industrial/Industrial_1428.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24298</th>\n",
       "      <td>AnnualCrop/AnnualCrop_2571.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>AnnualCrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24299</th>\n",
       "      <td>River/River_1418.jpg</td>\n",
       "      <td>8</td>\n",
       "      <td>River</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24300 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Filename  Label  \\\n",
       "0                          AnnualCrop/AnnualCrop_142.jpg      0   \n",
       "1      HerbaceousVegetation/HerbaceousVegetation_2835...      2   \n",
       "2                   PermanentCrop/PermanentCrop_1073.jpg      6   \n",
       "3                          Industrial/Industrial_453.jpg      4   \n",
       "4      HerbaceousVegetation/HerbaceousVegetation_1810...      2   \n",
       "...                                                  ...    ...   \n",
       "24295                           SeaLake/SeaLake_1943.jpg      9   \n",
       "24296                      AnnualCrop/AnnualCrop_211.jpg      0   \n",
       "24297                     Industrial/Industrial_1428.jpg      4   \n",
       "24298                     AnnualCrop/AnnualCrop_2571.jpg      0   \n",
       "24299                               River/River_1418.jpg      8   \n",
       "\n",
       "                  ClassName  \n",
       "0                AnnualCrop  \n",
       "1      HerbaceousVegetation  \n",
       "2             PermanentCrop  \n",
       "3                Industrial  \n",
       "4      HerbaceousVegetation  \n",
       "...                     ...  \n",
       "24295               SeaLake  \n",
       "24296            AnnualCrop  \n",
       "24297            Industrial  \n",
       "24298            AnnualCrop  \n",
       "24299                 River  \n",
       "\n",
       "[24300 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dir = pd.concat([dir_train, dir_valid], ignore_index=False)\n",
    "img_dir = img_dir.iloc[:,1:-1].reset_index().drop(['index'], axis=1)\n",
    "print(img_dir.shape)\n",
    "# img_dir = img_dir.iloc[:1000, :] # limit sample size when testing\n",
    "img_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels/classes\n",
    "images = []\n",
    "classes = []\n",
    "# labels.typeof()\n",
    "\n",
    "base_path = 'dataset/EuroSAT/'\n",
    "for index, row in img_dir.iterrows():\n",
    "    img_path = os.path.join(base_path, row['Filename'])\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    images.append(img)\n",
    "    classes.append(row['ClassName'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "images = np.array(images)/255.0\n",
    "labels = pd.get_dummies(classes).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24300, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(images.shape)\n",
    "images[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set up the data augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "valid_data_generator = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1024)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,494,858\n",
      "Trainable params: 265,482\n",
      "Non-trainable params: 3,229,376\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(res=224, num_classes=10):\n",
    "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(res, res, 3))\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            base_model,\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.5),\n",
    "            keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_1 = get_model()\n",
    "model_1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "608/608 [==============================] - 132s 214ms/step - loss: 0.8485 - accuracy: 0.8328 - val_loss: 0.6789 - val_accuracy: 0.8680\n",
      "Epoch 2/10\n",
      "608/608 [==============================] - 125s 206ms/step - loss: 0.5918 - accuracy: 0.8805 - val_loss: 0.4511 - val_accuracy: 0.9156\n",
      "Epoch 3/10\n",
      "608/608 [==============================] - 125s 205ms/step - loss: 0.5183 - accuracy: 0.8865 - val_loss: 0.4132 - val_accuracy: 0.9120\n",
      "Epoch 4/10\n",
      "608/608 [==============================] - 129s 212ms/step - loss: 0.4831 - accuracy: 0.8866 - val_loss: 0.4245 - val_accuracy: 0.9005\n",
      "Epoch 5/10\n",
      "608/608 [==============================] - 125s 205ms/step - loss: 0.4622 - accuracy: 0.8873 - val_loss: 0.4108 - val_accuracy: 0.9042\n",
      "Epoch 6/10\n",
      "608/608 [==============================] - 124s 204ms/step - loss: 0.4458 - accuracy: 0.8879 - val_loss: 0.3896 - val_accuracy: 0.9069\n",
      "Epoch 7/10\n",
      "608/608 [==============================] - 119s 196ms/step - loss: 0.4342 - accuracy: 0.8918 - val_loss: 0.4060 - val_accuracy: 0.9029\n",
      "Epoch 8/10\n",
      "608/608 [==============================] - 119s 196ms/step - loss: 0.4397 - accuracy: 0.8907 - val_loss: 0.3531 - val_accuracy: 0.9174\n",
      "Epoch 9/10\n",
      "608/608 [==============================] - 120s 197ms/step - loss: 0.4375 - accuracy: 0.8902 - val_loss: 0.3529 - val_accuracy: 0.9191\n",
      "Epoch 10/10\n",
      "608/608 [==============================] - 120s 197ms/step - loss: 0.4273 - accuracy: 0.8891 - val_loss: 0.3522 - val_accuracy: 0.9156\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_1.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "validation_steps = len(X_test) // batch_size\n",
    "\n",
    "history = model_1.fit(train_data_generator.flow(X_train, y_train, batch_size=batch_size), validation_data=valid_data_generator.flow(X_test, y_test, batch_size=batch_size), validation_steps=validation_steps, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenet_1.00_224_input with unsupported characters which will be renamed to mobilenet_1_00_224_input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 27). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_MobileNet_nFull_epoch10_batch32\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_MobileNet_nFull_epoch10_batch32\\assets\n"
     ]
    }
   ],
   "source": [
    "model_1.save(\"model_MobileNet_nFull_epoch10_batch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if want to load later on: \n",
    "from tensorflow.keras.models import load_model\n",
    "model_1 = load_model(\"model_MobileNet_nFull_epoch10_batch32\")\n",
    "\n",
    "\n",
    "# ## Alternative way - only save the weights\n",
    "# model_loaded = get_model()\n",
    "# model_loaded.load_weights(\"model_EfficientNetB0_nFull_epoch10_batch32_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "batch_size = 32  # Choose a smaller batch size according to your GPU memory capacity\n",
    "y_pred_batches = []\n",
    "\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    batch_pred = model_1.predict(X_test[i:i + batch_size])\n",
    "    y_pred_batches.append(batch_pred)\n",
    "\n",
    "y_pred = np.concatenate(y_pred_batches, axis=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# y_pred = model_1.predict(X_test)\n",
    "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# y_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93       551\n",
      "           1       0.93      0.92      0.93       547\n",
      "           2       0.87      0.92      0.89       535\n",
      "           3       0.79      0.93      0.86       459\n",
      "           4       0.98      0.95      0.96       453\n",
      "           5       0.87      0.91      0.89       371\n",
      "           6       0.90      0.87      0.89       456\n",
      "           7       0.96      0.98      0.97       502\n",
      "           8       0.94      0.79      0.86       469\n",
      "           9       0.94      0.98      0.96       517\n",
      "\n",
      "    accuracy                           0.92      4860\n",
      "   macro avg       0.92      0.91      0.91      4860\n",
      "weighted avg       0.92      0.92      0.92      4860\n",
      "\n",
      "[[488   0   1  12   0  11  22   0  11   6]\n",
      " [  1 503  11   1   0   8   0   0   0  23]\n",
      " [  0  13 490  10   0   8  11   3   0   0]\n",
      " [  4   1   2 429   2   5   4   2  10   0]\n",
      " [  0   0   0   8 430   0   4  11   0   0]\n",
      " [  0  11  14   5   0 338   1   0   1   1]\n",
      " [  4   0  36   8   4   3 397   4   0   0]\n",
      " [  0   0   5   0   3   0   0 494   0   0]\n",
      " [  4   6   3  67   2  14   1   1 371   0]\n",
      " [  0   5   0   1   0   1   0   0   2 508]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred_classes))\n",
    "print(confusion_matrix(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# batch_size = 32  # Choose a smaller batch size according to your GPU memory capacity\n",
    "# y_pred_batches = []\n",
    "\n",
    "# for i in range(0, len(X_test), batch_size):\n",
    "#     batch_pred = model_loaded.predict(X_test[i:i + batch_size])\n",
    "#     y_pred_batches.append(batch_pred)\n",
    "\n",
    "# y_pred = np.concatenate(y_pred_batches, axis=0)\n",
    "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# # y_pred = model_1.predict(X_test)\n",
    "# # y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# # y_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If you want to save the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_1.save(\"model_1\")\n",
    "\n",
    "# ## if want to load later on: \n",
    "# from tensorflow.keras.models import load_model\n",
    "# loaded_model = load_model(\"model_1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft / Archive (Plz disregard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt # somehow this just doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing/debugging\n",
    "# print(classes[0])\n",
    "# images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Use this when running for 1st time\n",
    "# MODEL_PATH = \"https://tfhub.dev/sayakpaul/convnext_base_21k_1k_224_fe/1\"\n",
    "# model = hub.load(MODEL_PATH)\n",
    "# # saved_model_path = 'model/'\n",
    "# # tf.saved_model.save(model, saved_model_path)\n",
    "\n",
    "# # ## Use this for subsequent runs when model already loaded locally\n",
    "# # saved_model_path = 'model/'\n",
    "# # model = tf.saved_model.load(saved_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
